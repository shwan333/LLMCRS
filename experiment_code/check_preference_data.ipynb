{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import statistics\n",
    "\n",
    "# load files from specified_directory\n",
    "no_tuned_dir =\"/home/shchoi/iEvaLM-CRS/save_5/user_Llama-3.1-8B-Instruct/emb_nomic-ai/nomic-embed-text-v1/openmodel_Llama-3.2-1B-Instruct_top10_full_history/opendialkg_eval/full_non_repeated/dpo_train_data_temp1.0_sample_num8_top10_reward_func_topK10\"\n",
    "tuned_dir =\"/home/shchoi/iEvaLM-CRS/save_5/user_Llama-3.1-8B-Instruct/emb_nomic-ai/nomic-embed-text-v1-true-total-tuned/openmodel_Llama-3.2-1B-Instruct_top10_full_history/opendialkg_eval/full_non_repeated/dpo_train_data_temp1.0_sample_num8_top10_reward_func_topK10\"\n",
    "# dir = \"/home/work/shchoi/iEvaLM-CRS/save_5/user_Llama-3.1-8B-Instruct/emb_nomic-ai/nomic-embed-text-v1-tuned-total/openmodel_Llama-3.2-1B-Instruct_top10_full_history/opendialkg_eval/full_non_repeated/dpo_train_data_temp1.0_sample_num8_top10\"\n",
    "# dir = \"/home/work/shchoi/iEvaLM-CRS/save_5/eval_noise/back_openai_model/chat/back_pervious_user_prompt/openmodel_Llama-3.2-1B-Instruct/opendialkg_eval/full_non_repeated/dpo_data\"\n",
    "\n",
    "def show_reward_distribution(dir: str) -> list:\n",
    "    \n",
    "    chosen_scores = []\n",
    "    rejected_scores = []\n",
    "    score_gaps = []\n",
    "    diff_scores = []\n",
    "\n",
    "    for file in os.listdir(dir):\n",
    "        if file.endswith(\".json\"):\n",
    "            data = json.load(open(os.path.join(dir, file), \"r\"))\n",
    "            if data[\"chosen_score\"] == data[\"rejected_score\"]:\n",
    "                print(file.split(\"/\")[-1])\n",
    "            chosen_scores.append(data[\"chosen_score\"])\n",
    "            rejected_scores.append(data[\"rejected_score\"])\n",
    "            score_gaps.append(abs(data[\"chosen_score\"] - data[\"rejected_score\"]))\n",
    "            diff_scores.append(data[\"chosen_score\"] - data[\"rejected_score\"])\n",
    "    \n",
    "    print(\"chosen_scores\")\n",
    "    print(f\"mean: {statistics.mean(chosen_scores):.3f}, std: {statistics.stdev(chosen_scores):.3f}\")\n",
    "\n",
    "    print(\"rejected_scores\")\n",
    "    print(f\"mean: {statistics.mean(rejected_scores):.3f}, std: {statistics.stdev(rejected_scores):.3f}\")\n",
    "\n",
    "    print(\"gap\")\n",
    "    print(f\"mean: {statistics.mean(score_gaps):.3f}, std: {statistics.stdev(score_gaps):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned_dir\n",
      "chosen_scores\n",
      "mean: 0.089, std: 0.108\n",
      "rejected_scores\n",
      "mean: -0.024, std: 0.083\n",
      "gap\n",
      "mean: 0.114, std: 0.093\n",
      "==================================\n",
      "no_tuned_dir\n",
      "3066_1_2.json\n",
      "748_1_2.json\n",
      "1711_1_4.json\n",
      "chosen_scores\n",
      "mean: 0.019, std: 0.023\n",
      "rejected_scores\n",
      "mean: -0.007, std: 0.019\n",
      "gap\n",
      "mean: 0.026, std: 0.021\n"
     ]
    }
   ],
   "source": [
    "print(\"tuned_dir\")\n",
    "show_reward_distribution(tuned_dir)\n",
    "print(\"==================================\")\n",
    "print(\"no_tuned_dir\")\n",
    "show_reward_distribution(no_tuned_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shchoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
